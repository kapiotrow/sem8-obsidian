
## 1. BezpoÅ›rednie metody obliczeniowe: Optymalizacja czasÃ³w przeÅ‚Ä…czeÅ„

### ğŸ§  Zasada dziaÅ‚ania

RozwaÅ¼amy sterowanie optymalne, w ktÃ³rym funkcja sterujÄ…ca przyjmuje wartoÅ›ci kraÅ„cowe:

$$ u(t) \in \{u_{\min}, u_{\max}\}$$ Sterowanie tego typu nazywa siÄ™ **brzegowym** lub **bang-bang**. Decyzjami nie sÄ… wartoÅ›ci funkcji $u(t)$, ale **czasy przeÅ‚Ä…czeÅ„** $$\tau = (\tau_1, \tau_2, \dots, \tau_M)$$, gdzie $u$ zmienia siÄ™ z $u_{\min}$ na $u_{\max}$ lub odwrotnie. System dynamiczny ma postaÄ‡:

$$
\dot{x}(t) = f_0(x(t)) + f_1(x(t))u(t), \quad x(0) = x_0
$$
FunkcjaprzeÅ‚Ä…czajÄ…ca: Funkcja przeÅ‚Ä…czajÄ…ca:
$$
\varphi(t) = \psi^T(t) f_1(x(t))
$$
Warunekmaksimum: Warunek maksimum:
$$
u(t) = \begin{cases}  
u_{\max} & \text{gdy } \varphi(t) < 0 \  
u_{\min} & \text{gdy } \varphi(t) > 0  
\end{cases}$$

### âš™ï¸ Algorytm

1. **Inicjalizacja**:
    
    - wybierz $u_0 \in {u_{\min}, u_{\max}}$
        
    - ustaw liczbÄ™ przeÅ‚Ä…czeÅ„ $M = 0$
        
2. **Optymalizacja**:
    
    - dla danego $M$ minimalizuj wskaÅºnik Î£(u0,Ï„1,â€¦,Ï„M)\Sigma(u_0, \tau_1, \dots, \tau_M) np. metodÄ… BFGS.
        
    - wylicz gradient:
        $$
        âˆ‚Î£âˆ‚Ï„i=H(x(Ï„iâˆ’),Ïˆ(Ï„iâˆ’),uiâˆ’1)âˆ’H(x(Ï„i+),Ïˆ(Ï„i+),ui)\frac{\partial \Sigma}{\partial \tau_i} = H(x(\tau_i^-), \psi(\tau_i^-), u_{i-1}) - H(x(\tau_i^+), \psi(\tau_i^+), u_i) $$
    - bierz kierunek $d$ i wyznacz maksymalny krok $s_{\max}$ speÅ‚niajÄ…cy warunki nierÃ³wnoÅ›ci.
        
3. **Warunek sprawdzajÄ…cy**:
    
    - czy sterowanie speÅ‚nia warunek maksimum?
        
        - jeÅ›li nie: generacja szpilkowa
            
        - jeÅ›li tak: koniec
            
4. **Generacja szpilkowa**:
    
    - dodaj nowe przeÅ‚Ä…czenie w miejscu minimum $$2u(t)âˆ’uminâ¡âˆ’umaxâ¡âˆ£â‹…âˆ£Ï†(t)âˆ£|2u(t) - u_{\min} - u_{\max}| \cdot |\varphi(t)$$
        
5. **Redukcja**:
    
    - usuÅ„ przeÅ‚Ä…czenie, jeÅ›li nie wnosi poprawy lub leÅ¼y na brzegu dopuszczalnego obszaru
        

### âœ… Zalety

- przejrzysta struktura,
    
- moÅ¼liwoÅ›Ä‡ uÅ¼ycia efektywnych metod optymalizacji (BFGS, L-BFGS-B).
    

### âŒ Wady

- wymaga dobrej inicjalizacji,
    
- trudnoÅ›ci przy optymalizacji liczby przeÅ‚Ä…czeÅ„.
    

---

## 2. Prosta poÅ›rednia metoda strzaÅ‚Ã³w (Rozdz. 12)

### ğŸ§  Zasada dziaÅ‚ania

RozwiÄ…zujemy dwupunktowy problem brzegowy dla ukÅ‚adu kanonicznego:
$$
zË™(t)=F(z(t))=[f(x(t),u(t))âˆ’âˆ‚Hâˆ‚x(x(t),u(t),Ïˆ(t))],z(0)=[x0p]\dot{z}(t) = F(z(t)) = \begin{bmatrix} f(x(t), u(t)) \\ -\frac{\partial H}{\partial x}(x(t), u(t), \psi(t)) \end{bmatrix}, \quad z(0) = \begin{bmatrix}x_0 \\ p\end{bmatrix}
$$
Warunek koÅ„cowy:
$$
Ïˆ(T)+âˆ‡q(x(T))=0â‡’Î¦(p)=0\psi(T) + \nabla q(x(T)) = 0 \quad \Rightarrow \Phi(p) = 0
$$
### âš™ï¸ Algorytm

1. **Zainicjuj** $p = p_0$
    
2. **Iteracja Newtona**:
    $$
    p(k+1)=p(k)âˆ’[Î¦â€²(p(k))]âˆ’1Î¦(p(k))p^{(k+1)} = p^{(k)} - [\Phi'(p^{(k)})]^{-1} \Phi(p^{(k)})
    $$
3. **Obliczenie pochodnej**:
    
    - podejÅ›cie wariacyjne: integracja ukÅ‚adu
        $$
        Î¾Ë™(t)=âˆ‚Fâˆ‚z(z(t))Î¾(t),Î¾(0)=I\dot{\xi}(t) = \frac{\partial F}{\partial z}(z(t)) \xi(t), \quad \xi(0) = I
        $$
    - podejÅ›cie sprzÄ™Å¼one:
        $$
        Î·Ë™(t)=âˆ’(âˆ‚Fâˆ‚z(z(t)))TÎ·(t),Î·(T)=[âˆ‡q(x(T))I]\dot{\eta}(t) = -\left(\frac{\partial F}{\partial z}(z(t))\right)^T \eta(t), \quad \eta(T) = \begin{bmatrix} \nabla q(x(T)) \\ I \end{bmatrix}
        $$
    - wtedy: $Î¦â€²(p)=Î·T(0)\Phi'(p) = \eta^T(0)$
        
4. **ZbieÅ¼noÅ›Ä‡**: gdy $|\Phi(p)| < \varepsilon$
    

### âœ… Zalety

- szybka zbieÅ¼noÅ›Ä‡ blisko optimum
    

### âŒ Wady

- wymaga dobrej inicjalizacji $p$
    
- czuÅ‚oÅ›Ä‡ na bÅ‚Ä™dy numeryczne
    

---

## 3. PoÅ›rednia metoda strzaÅ‚Ã³w wielokrotnych (Rozdz. 12)

### ğŸ§  Zasada dziaÅ‚ania

Zamiast jednego dÅ‚ugiego problemu brzegowego, dzieli siÄ™ przedziaÅ‚ $[0, T]$ na $N$ mniejszych i rozwiÄ…zuje ukÅ‚ady lokalnie z warunkami dopasowania:
$$
zi(ti+)=zi+1(ti+)z_i(t_i^+) = z_{i+1}(t_i^+)
$$
### âš™ï¸ Algorytm

1. **PodziaÅ‚ czasu**: $t_0 = 0 < t_1 < \dots < t_N = T$
    
2. **Na kaÅ¼dym przedziale**:
    
    - solve: $\dot{z}_i = F(z_i),\ z_i(t_i) = \sigma_i$
        
3. **Warunki dopasowania**: $\sigma_{i+1} = z_i(t_{i+1})$
    
4. **RozwiÄ…zywanie caÅ‚ego ukÅ‚adu** jako systemu rÃ³wnaÅ„ algebraicznych metodÄ… Newtona.
    

### âœ… Zalety

- rozszerzony obszar zbieÅ¼noÅ›ci
    
- lepsza stabilnoÅ›Ä‡ numeryczna
    

### âŒ Wady

- wysoki koszt obliczeniowy
    

---

## 4. Metoda Borelowskiego â€“ Minimalnoczasowa (Rozdz. 13)

### ğŸ§  Zasada dziaÅ‚ania

Dla systemu z ograniczonym sterowaniem:
$$
xË™(t)=f0(x(t))+f1(x(t))u(t),u(t)âˆˆ[âˆ’umaxâ¡,umaxâ¡]\dot{x}(t) = f_0(x(t)) + f_1(x(t))u(t), \quad u(t) \in [-u_{\max}, u_{\max}]
$$
Hamiltonian:
$$
H(x,u,Ïˆ)=ÏˆTf0(x)+ÏˆTf1(x)uH(x, u, \psi) = \psi^T f_0(x) + \psi^T f_1(x) u
$$
Sterowanie brzegowe:
$$
u(t) = u_{\max} \cdot \text{sgn}(\psi^T f_1(x))$$ ### âš™ï¸ Algorytm (wariant najprostszy) 1. **ZaÅ‚oÅ¼enie liczby przeÅ‚Ä…czeÅ„ $k = n - 1$** 2. **Zmienna decyzyjna**: wektor czasÃ³w przeÅ‚Ä…czeÅ„ $\tau = (\tau_1, \dots, \tau_k)$ 3. **RÃ³wnanie dopasowania**:
$$
\Phi(\tau) = x(T; \tau) - x_f = 0
$$
4.âˆ—âˆ—MetodaNewtonaâˆ—âˆ—:4. **Metoda Newtona**:
$$
\tau^{(i+1)} = \tau^{(i)} - [\Phi'(\tau^{(i)})]^{-1} \Phi(\tau^{(i)})
$$
5. **Wyznaczenie pochodnych**: przez ukÅ‚ady wariacyjne lub scaÅ‚kowanie macierzy fundamentalnej wstecz. ### âœ… Zalety - przystosowana do zadaÅ„ minimalnoczasowych - pozwala uwzglÄ™dniÄ‡ warunek maksimum ### âŒ Wady - niestabilnoÅ›Ä‡ numeryczna - metoda dziaÅ‚a dobrze tylko przy dobrej inicjalizacji --- ## 5. Generacje szpilkowe i redukcje przeÅ‚Ä…czeÅ„ ### Generacje szpilkowe - dodanie przeÅ‚Ä…czeÅ„ w miejscach, gdzie funkcja przeÅ‚Ä…czajÄ…ca silnie narusza warunek maksimum:
$$
\varphi(t) = \psi^T(t) f_1(x(t)) \not= 0
$$
- nowe czasy przeÅ‚Ä…czeÅ„ dodaje siÄ™ w parze (np. $\tau_k = \theta$, $\tau_{k+1} = \theta$) ### Redukcje przeÅ‚Ä…czeÅ„ - usuniÄ™cie przeÅ‚Ä…czenia, gdy: - nie poprawia wskaÅºnika - jest nieaktywny w sensie warunku maksimum --- W razie potrzeby mogÄ™ dopisaÄ‡ przykÅ‚ady numeryczne lub kod implementacyjny w Pythonie lub MATLABie.