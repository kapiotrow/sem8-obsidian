## Ogólny pomysł
Stosowanie operacji konwolucji (nałożenie filtra) aby uzyskać lokalne cechy, połączenie fragmentu warstwy wejściowej do jednego neuronu następnej warstwy (struktura przestrzenna). Efektem przesunięcia filtra po obrazie są mapy aktywacji. Po każdej konwolucji powinno się wprowadzić nieliniowość (np. ReLU), a następnie pooling w celu zmniejszenia rozmiaru danych i zachowania niezmienności przestrzennej. Ostatnim krokiem jest klasyfikacja, np.: flatten -> fully connected -> softmax.
* stride - krok o jaki przesuwa się okno filtra
* padding - dodawanie sztucznych pikseli wokół obrazu wejściowego (np. zer)
* warstwy pooling - zmniejszają rozmiar danych; maxpool, avgpool

## Architektury
### AlexNet
* pierwsze użycie ReLU
* Dropout 0.5
* Batch size 128
* 8 warstw

### VGGNet
* mniejsze filtry, głębsza sieć - więcej nieliniowości

### GoogleNet
* 22 warstwy
* moduł "incepcji"
 ![[Pasted image 20250614181443.png]]
 * używa konwolucji 1x1 do redukcji głębokości cech
### ResNet
* bardzo głęboka sieć
* połączenia rezydualne
![[Pasted image 20250614181803.png]]
* głębsze modele są cięższe do wytrenowania - problem optymalizacji
* tak jak GoogleNet, używa warstw bottleneck 
* bez dropoutu

