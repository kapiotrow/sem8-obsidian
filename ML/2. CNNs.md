## Ogólny pomysł
Stosowanie operacji konwolucji (nałożenie filtra) aby uzyskać lokalne cechy, połączenie fragmentu warstwy wejściowej do jednego neuronu następnej warstwy (struktura przestrzenna). Efektem przesunięcia filtra po obrazie są mapy aktywacji. Po każdej konwolucji powinno się wprowadzić nieliniowość (np. ReLU), a następnie pooling w celu zmniejszenia rozmiaru danych i zachowania niezmienności przestrzennej. Ostatnim krokiem jest klasyfikacja, np.: flatten -> fully connected -> softmax.
* stride - krok o jaki przesuwa się okno filtra
* padding - dodawanie sztucznych pikseli wokół obrazu wejściowego (np. zer)
* warstwy pooling - zmniejszają rozmiar danych; maxpool, avgpool
* "unpooling" - upsampling
* odwrotna konwolucja

### Segmentacja
Sieć konwolucyjna z downsamplingiem i upsamplingiem. Do wielu instancji Mask R-CNN. 
U-Net - 

### Detekcja
* przesuwające się okno - zbyt wolne
* region proposal - szybsze
* R-CNN - RoI są wkładane do ConvNetów, klasyfikacja następuje w SVM-ach, regresja liniowa do bboxów; długo się uczy, detekcja trwa długo
* Fast R-CNN - cały obraz wejściowy wkładamy do ConvNeta, wybieramy RoI-e z mapy cech będącej wyjściem z ConvNeta, RoI-e się wkłada do warstw w pełni połączonych, na koniec klasyfikator softmax i regresja liniowa do bboxów
* Detekcja bez propozycji (YOLO) - dzieli obraz na siatkę

## Architektury
### AlexNet
* pierwsze użycie ReLU
* Dropout 0.5
* Batch size 128
* 8 warstw

### VGGNet
* mniejsze filtry, głębsza sieć - więcej nieliniowości

### GoogleNet
* 22 warstwy
* moduł "incepcji"
 ![[Pasted image 20250614181443.png]]
 * używa konwolucji 1x1 do redukcji głębokości cech
### ResNet
* bardzo głęboka sieć
* połączenia rezydualne
![[Pasted image 20250614181803.png]]
* głębsze modele są cięższe do wytrenowania - problem optymalizacji
* tak jak GoogleNet, używa warstw bottleneck 
* bez dropoutu

### DenseNet
Utylizuje **gęste połączenia** pomiędzy warstwami. Każda warstwa otrzymuje dodatkowe wejścia ze wszystkich poprzednich warstw i przekazuje swoje wyjście do wszystkich następnych warstw.

### MobileNet
Rodzina sieci konwolucyjnych stworzona z myślą o klasyfikacji i detekcji dla systemów o małym rozmiarze, poborze mocy i niskich latencjach (edge computing).

### EfficientNet
Rodzina sieci konwolucyjnych stworzona przez Google AI. Kluczowa innowacja: **compound scaling**, czyli jednoczesne skalowanie sieci we wszystkich trzech wymiarach. 

### ConvNeXt
Sieć konwolucyjna wprowadzająca innowacje zproponowane przez sieci transformerowe. Używa wartsw typu Global Average Pooling, która spłaszcza wyjście warstw konwolucyjnych do postaci wektora, który może być użyty do celu klasyfikacji czy regresji w warstwach pełni połączonych.
* małe filtry - mniej parametrów, mniej obliczeń
* połączenia długodystansowe - bezpośrednie połączenia wyjść warstw do wejść następnych warstw
* normalizacja grupy zamiast normalizacja warstwy
* funkcja aktywacji - GELU (nieliniowe wyjście dla wartości ujemnych)