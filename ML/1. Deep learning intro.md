### Głębokie uczenie 
Wiele warstw realizujących nieliniowe transformacje, które tworzą model hierarchiczny (warstwy odwzorowują kolejne poziomy abstrakcji). Model przeprowadza zarówno ekstrakcję cech, jak i klasyfikację.
**Uczenie maszynowe** - ekstrakcja cech prowadzona poza modelem klasyfikującym, najczęściej ręcznie przez człowieka.
**Głębokie vs płytkie uczenie** - płytkie do 2 warstw i wymagają ręcznie przygotowanych cech, łatwiej się przetrenowują, wymagają więcej neuronów; głębokie więcej niż 2 warstwy przetwarzania, model hierarchiczny.

**Problemy**
* Zanikający gradient
* Ograniczone funkcje aktywacji nasycają się i posiadają niezerowy gradient tylko w wąskim przedziale aktywacji bliskim 0
* Przeuczenie - przeciwdziałają mu techniki regularycacji
* Większa liczba parametrów + dużo danych -> długi czas uczenia, duże wymagania dot. pamięci.

### Dropout
Dla każdego wzorca treningowego z prawdopodobieństwem $p$ wyrzuca jednostki (zeruje aktywacje). 
* W czasie ewaluacji dropout jest wyłączony
* Każdy krok uczenia odbywa się ze zmienioną losowo architekturą, ale wagi są współdzielone
* Przeciwdziała powstawaniu złożonych relacji pomiędzy wieloma neuronami; pojedynczy neuron jest zmuszony wykrywać bardziej wartościowe cechy niezależnie od wpływu innych neuronów
* Wolniejsza zbieżność, ale zapobiega przeuczeniu.

### Batch normalization
Normalizuje wejścia warstwy względem wartości średniej i wariancji wzdłuż jednego z wymiarów.
* Przyspiesza trening
* Poprawia stabilność treningu
* Polepsza generalizację
* Rozwiązuje problem przesunięcia kowariancji - gdy zmienia się wyjście warstwy poprzedniej, warstwa następna musi dopasować się do nowego rozkładu danych
* Pozwala stosować większe kroki uczenia bez ryzyka wystąpienia zanikającego/eksplodującego gradientu

### Funkcje aktywacji
Wprowadzają element nieliniowości. 
* **Tanh** - "zero-centered", ale też się saturuje; raczej się nie używa
* **Sigmoid** - nie jest "zero-centered", saturuje się, exp jest wymagający obliczeniowo
* **ReLU** - nie saturuje się, mało wymagający obliczeniowo, bliższy działaniu prawdziwego neuronu; nie jest "zero-centered"
* **Maxout**
* **Leaky ReLU** - nie saturuje się, efektywny obliczeniowo, nie "umiera" jak zwykłe reLU
* **ELU** - wszystkie zalety ReLU, średnia wyjść bliższa zeru, nie "umiera", niestety wymaga exp :(


### Funkcje kosztu
Podczas treningu poszukiwane są wagi modelu, które minimalizują koszt. Po wybraniu punktu początkowego oblicza się gradient, a następnie "robi krok" w kierunku opadania gradientu i aktualizuje wagi; te dwie czynności są powtarzane aż do uzyskania odpowiedniej zbieżności.
* **MSE** - błąd średniokwadratowy; problemy aproksymacji, wyjścia ciągłe.
* **MAE** - mean absolute error; wyjścia ciągłe.
* **Binary cross-entropy** - klasyfikacja (wyjścia binarne), softmax lub sigmoidalne neurony wyjściowe.
* **Categorical cross-entropy** - dwie lub więcej klas, gdy etykiety są w formacie one-hot. 
* **Sparse categorical cross-entropy** - dwie lub więcej klas, etykiety będące liczbami całkowitymi.

### Metody regularyzacji
Przeciwdziałają przeuczaniu sieci, poprawia zdolności do generalizacji.
* $L_1$, $L_2$
* Early-stopping - zapobiega overfittingowi zatrzymując uczenie zanim wagi staną się zbyt duże
* [[#Dropout]]
* [[#Batch normalization]]
* Gradient clipping
* [[3. Data augmentation & Transfer Learning]]

### Adaptive learning rate algorithms
Rozwiązuje problem doboru stałego LR.
* Adagrad
* Adam
* Adadelta
* RMSProp
* Momentum