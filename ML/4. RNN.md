Przetwarza sekwencję wektorów x, poddając ją działaniu funkcji rekurencyjnej w każdej jednostce czasu. Ta sama funkcja i te same parametry (ta sama macierz wag) są używane w każdym kroku.

![[Pasted image 20250615111142.png]]

## Architektury
* jeden do jednego - vanilla NN
* jeden do wielu - podpisywanie obrazów
* wiele do jednego - sekwencja słów -> podsumowanie
* wiele do wielu - tłumaczenie

## LSTM - Long Short Term Memory
Kontroluje zanikanie gradientu. 
Four gates: forget, input, output and gate gate (how much to reveal cell). 

## GRU - Gated Recurrent Unit
Podobne do LSTM, ale są streamline'owane, przez co mniej wymagające obliczeniowo. Nie ma wektora kontekstu i bramki wyjściowej, ma mniej parametrów niż LSTM.

## Bidirectional GRU layer
Składa się z dwóch warstw rekurencyjnych, z których jedna przetwarza sekwencję chronologicznie, a druga od tyłu. 

## Przetwarzanie języka naturalnego
Sieci rekurencyjne są wykorzystywane do przetwarzania języka naturalnego.
### One-hot encoding
* wektor zer i jedynek o stałej długości, gdzie wystąpienie danego słowa jest oznaczone "1" na indeksie tego słowa
* taki słownik szybko puchnie, nie oddaje zależności pomiędzy słowami
* słowa o różnych końcówkach są traktowane jako zupełnie inne słowa
### Word embedding
* próba sparametryzowania znaczenia słów
* mapuje słowa na wektory w wielowymiarowej przestrzeni znaczenia
* dystanse, kierunki i zwroty odzwierciedlają wzajemne stosunki pomiędzy słowami

## Szeregi czasowe
Szeregi czasowe to dane uszeregowane chronologicznie - pomiary z czujnika temperatury, film. Ich przetwarzanie wymaga zbadania zależności pomiędzy kolejnymi próbkami i na ogół ma na celu predykcję przebiegu w przyszłości.
Ostatnia warstwa sieci, która przetwarza szereg czasowy, musi dawać ciągłe wyjście (Dense(1), bez funkcji aktywacji), a funkcja kosztu podczas treningu musi badać różnicę między przewidywaną a rzeczywistą wartością szeregu czasowego (np. MAE, MSE). 